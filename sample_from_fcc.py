# ------------------------------------------------------------------
# export of FCC filings using a download plan
# example usage for proceeding 17-108
# Usage:
# python getDownloadPlan.py {APIKey}
# ------------------------------------------------------------------
import json
import requests
import sys
import os
import time
import csv
import urllib.request
import pandas as pd
from pandas.io.json import json_normalize
from pathlib import Path

sys.path.append("classes")


from ExparteLog import ExparteLog
from ExparteBotoS3 import ExparteBotoS3


#### BEGIN SETUP LOGGING ####
log = "/webroot_logs/PythonLog.txt"
auth_csv_file = "/webroot_auth/dev_logger_auth.txt"
log_email_to = "exparte.errors.dev@gmail.com"
logger = ExparteLog(log, auth_csv_file, log_email_to)
#### END SETUP LOGGING ####

#set up the base directory and the download plan directory
base_dir = "/webroot_json_fcc/"
downloadplan_dir = "/webroot_json_fcc/downloadplan/"

#custom downloads
#original plan modified to a chopped file.
custom_download_plan = "/webroot_json_fcc/downloadplan/downloadplan_20180527_chopped2.txt"
custom_download_plan = "/webroot_json_fcc/downloadplan/downloadplan_20180619_chopped.txt"
#this was the original 1998-2018. impossible to download on laptop.
custom_download_plan = "/webroot_json_fcc/downloadplan/downloadplan_20180527.txt"
#this was used for fcc study.
custom_download_plan = "/webroot_json_fcc/downloadplan/downloadplan_2006-2016.txt"
custom_download_plan = "/webroot_json_fcc/downloadplan/downloadplan_20190307_2017.txt"


'''
#these were missing
fcc_0001450300740000_000000.json - 9000
fcc_0001451714520000_000000.json - 9000
fcc_0001457728680000_000000.json - 6000
fcc_0001513235640000_000000.json - 9000
fcc_0001513255920000_000000.json - 9000
fcc_0001513262580000_000000.json - 9000
fcc_0001513266600000_000000.json - 9000
fcc_0001513270080000_000000.json - 9000
fcc_0001513273200000_000000.json - 9000
fcc_0001513276380000_000000.json - 9000
fcc_0001513281000000_000000.json - 9000
fcc_0001513287720000_000000.json - 9000
fcc_0001513296240000_000000.json - 9000
fcc_0001513306560000_000000.json - 9000
fcc_0001513319820000_000000.json - 9000
fcc_0001513356600000_000000.json - 9000
fcc_0001513372320000_000000.json - 9000
fcc_0001513440540000_000000.json - 9000
fcc_0001513559640000_000000.json - 9000
fcc_0001514481720000_000000.json - 9000
fcc_0001519748940000_000000.json - 9000
'''


#1457728680000 missing from 2006-2016

#get api key
with open('/webroot_auth/FCCAPIKey.txt', 'rt') as csvfile:
	mysql_auth = csv.reader(csvfile, delimiter=',', quotechar='|')
	data = list(mysql_auth)
	api_key = data[1][0]
	
APIKey = "&api_key="+api_key

#open s3 class
bucket_name = "fcc-json-all"
s3 = ExparteBotoS3(bucket_name)

#########################################################
###### SELF-GENERATE A DOWNLOAD PLAN ####################
## MOTHER url
## This APIUrl below is used to generate the download plan that will be used by the script.
## This url says, give me all the filings ever, sorted by date received, ASC.
## Assumes that the filings will grow as they are entered.

## Uncomment this section when you want to download a new download plan.
## Leave it commented out if you have a download plan already.

'''
APIUrl = "https://publicapi.fcc.gov/ecfs/filings?api_key=" + api_key + "&sort=date_received,ASC&type=downloadplan"

#&submissiontype.description=NOTICE%20OF%20EXPARTE

print(APIUrl)
response = requests.get(APIUrl)
parsed = response.json()

#get download plan in json
filename = downloadplan_dir + 'downloadplan.json'
with open(downloadplan_dir + 'downloadplan.txt', 'w') as outfile:
	json.dump(parsed, outfile)
'''

#example of suggested_api_call generated by download
#https://publicapi.fcc.gov/ecfs//filings?sort=date_submission,ASC&date_submission=[gte]1988-12-23T05:00:00.000Z[lt]1999-01-28T21:58:00.000Z&limit=9999
#########################################################


#########################################################
############### CUSTOM DOWNLOAD PLAN ####################
## Use this section if you have prepared a download plan

with open(custom_download_plan) as f:
	parsed = json.load(f)

#print(type(parsed))
#print(parsed['aggregations']['download_plan']['buckets'])

#########################################################


count=0
div = 1000

## This fucntion will download and save the slice you have requested
def downloadAndSave(api_call_key, file_slice_key):
	print('--> ' + api_call_key)
	slice_req = requests.get(api_call_key, stream=True)
	with open(file_slice_key, 'wb') as fd:
		for chunk in slice_req.iter_content(chunk_size=128):
			if chunk:
				fd.write(chunk)
		fd.close()

#for each bucket iterate over list of suggested api calls.
for doc in parsed['aggregations']['download_plan']['buckets']:
	#note that the key is unix timestamp
	file_slice = str(doc['key'])
	#print(json.dumps(doc, indent=4,sort_keys=True))
	if doc['suggested_api_call']:
		for api_call in doc['suggested_api_call']:
			offset=0
			#split the files so that you can troubleshoot problems easier
			#split by the number specified in the variable div above
			key_items = int((doc['doc_count'] / div) + 1)
			print("The number of key_items for " + file_slice + " = " + str(key_items))
			
			#check if you've already downloaded this document key
			#last_item = div * (key_items - 1)
			#last_file = base_dir + "fcc_" + str(doc['key']) + "_" +str(last_item).zfill(6) +'.txt'
			#print("the last file in this series is " + last_file)
			#my_file = Path(last_file)
			#if my_file.is_file():
			#	print("skipping " + str(doc['key']) + " because found last file in the series: " + last_file)
				#add check to see if it has the right number of submissions.
			#	break
			
			#debug with this stopper
# 			if count > 1:
# 				break
# 			print("the count is " + str(count))

			#split at limit so you can add a new limit
			#this chops off limit from the end of the url.
			matches = api_call.split("&limit=")
			print(matches)
			try:
				#this is the new url without the limit.
				api_call_base = matches[0]
				#key items from above
				for x in range(0, key_items):
					print("key item = " + str(x))
					print("the offset is = " + str(offset))
					print("the div is = " + str(div))
					mini_api_call = api_call_base + "&limit=" + str(div) + "&offset=" + str(offset)
					this_filename = "fcc_" + str(doc['key']).zfill(16) + "_" +str(offset).zfill(6) +'.json'
					


					root_folder = ""
					found_on_s3 = s3.searchForOneFile(this_filename)
					
					if not found_on_s3:
						file_slice_key = base_dir + this_filename
						api_call_key=mini_api_call+APIKey
						downloadAndSave(api_call_key, file_slice_key)
						offset = offset + div

			except BaseException as e: # catch *all* exceptions
				exc_type, exc_value, exc_traceback = sys.exc_info()	
				error_message = str(exc_type) + " " + str(exc_value)
				print(str(error_message))

				#api_call_key=api_call+APIKey
				#file_slice_key = base_dir + "fcc_" + str(doc['key']) + "_" +str(doc['doc_count'])+'.json'
				#downloadAndSave(api_call_key, file_slice_key)
				logger.writeLog("ERROR: couldn't split api_call at limit", str(error_message))
			count = count + 1
			
			if not found_on_s3:
				#### Sleep for 10 minutes because API gets unhappy if you call it constantly
				print("sleeping")
				#time in seconds
				#time.sleep(5);
				#sleep for 5 minutes
				time.sleep(300);

# End While

# Save  the  file